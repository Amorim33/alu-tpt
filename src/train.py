import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import torch
import torch.nn as nn
from torch.utils.data import random_split, DataLoader

from datasets import load_dataset
from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm

from pathlib import Path

from config import get_config, get_weights_file_path
from dataset import TranslationDataset, causal_mask
from model import build_transformer

def run_test(model, test_dataset, src_tokenizer, tgt_tokenizer, max_len, device, print_msg, num_examples=2):
    model.eval()
    count = 0
    console_width = 80 

    with torch.no_grad():
        for batch in test_dataset:
            count += 1
            encoder_input = batch['encoder_input'].to(device)
            encoder_mask = batch['encoder_mask'].to(device)

            sos_idx = src_tokenizer.token_to_id("[SOS]")
            eos_idx = src_tokenizer.token_to_id("[EOS]")

            # Precompute the encoder output and reuse it for every token generated by the decoder
            encoder_output = model.encode(encoder_input, encoder_mask)
            
            decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(encoder_input).to(device)
            while decoder_input.size(1) != max_len:
                decoder_mask = causal_mask(decoder_input.size(1)).type_as(encoder_mask).to(device)
                decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)

                proj_output = model.project(decoder_output[:, -1])
                _, next_token = torch.max(proj_output, dim=-1)

                decoder_input = torch.cat(
                    [decoder_input, torch.empty(1, 1).type_as(encoder_input).fill_(next_token.item()).to(device)], dim=1
                )

                if next_token == eos_idx:
                    break
            
            model_output = decoder_input.squeeze(0)

            src_text = batch['src_text'][0]
            tgt_text = batch['tgt_text'][0]
            model_output_text = tgt_tokenizer.decode(model_output.detach().cpu().numpy())

            print_msg('-'*console_width)
            print_msg(f"Source: {src_text}")
            print_msg(f"Expected: {tgt_text}")
            print_msg(f"Predicted: {model_output_text}")

            if count == num_examples:
                break



def get_all_sentences(dataset, lang):
    for item in dataset:
        yield item['translation'][lang]

def get_or_build_tokenizer(config, dataset, lang):
    tokenizer_path = Path(config['tokenizer_path'].format(lang))
    if not Path.exists(tokenizer_path):
        tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
        tokenizer.pre_tokenizer = Whitespace()
        trainer = BpeTrainer(special_tokens=["[UNK]", "[PAD]", "[SOS]", "[EOS]"], min_frequency=2)
        tokenizer.train_from_iterator(get_all_sentences(dataset, lang), trainer=trainer)
        tokenizer.save(str(tokenizer_path))
    else:
        tokenizer = Tokenizer.from_file(str(tokenizer_path))

    return tokenizer

def get_dataset(config):
    dataset = load_dataset('Helsinki-NLP/news_commentary', f'{config["src_lang"]}-{config["tgt_lang"]}', split='train')
    src_tokenizer = get_or_build_tokenizer(config, dataset, config['src_lang'])
    tgt_tokenizer = get_or_build_tokenizer(config, dataset, config['tgt_lang'])

    # split the dataset into train and test (90% train, 10% test)
    train_size = int(0.9 * len(dataset))
    test_size = len(dataset) - train_size
    train_dataset_raw, test_dataset_raw = random_split(dataset, [train_size, test_size])

    train_dataset = TranslationDataset(train_dataset_raw, src_tokenizer, tgt_tokenizer, config['src_lang'], config['tgt_lang'], config['seq_len'])
    test_dataset = TranslationDataset(test_dataset_raw, src_tokenizer, tgt_tokenizer, config['src_lang'], config['tgt_lang'], config['seq_len'])

    src_max_len = max(len(src_tokenizer.encode(item['translation'][config['src_lang']]).ids) for item in dataset)
    tgt_max_len = max(len(tgt_tokenizer.encode(item['translation'][config['tgt_lang']]).ids) for item in dataset)
    print(f"Max source length: {src_max_len}")
    print(f"Max target length: {tgt_max_len}")

    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)

    return train_loader, test_loader, src_tokenizer, tgt_tokenizer

def get_model(config, vocab_src_len, vocab_tgt_len):
    return build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])
    
def train_model(config):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Initialize GradScaler for mixed precision training
    scaler = torch.amp.GradScaler()

    Path(config['model_folder']).mkdir(exist_ok=True)

    train_loader, test_loader, src_tokenizer, tgt_tokenizer = get_dataset(config)
    model = get_model(config, src_tokenizer.get_vocab_size(), tgt_tokenizer.get_vocab_size()).to(device)
    writer = SummaryWriter(config['experiment_name'])
    
    # Define warmup parameters
    warmup_steps = config.get('warmup_steps', 4000)
    base_lr = config['lr']
    
    # Initialize optimizer with a very small learning rate
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-7, eps=1e-9)

    # Add gradient accumulation steps
    gradient_accumulation_steps = config.get('gradient_accumulation_steps', 1)
    print(f"Gradient accumulation steps: {gradient_accumulation_steps}")
    print(f"Warmup steps: {warmup_steps}")
    
    max_grad_norm = config.get('max_grad_norm', 1.0)

    initial_epoch = 0
    global_step = 0
    if config['preload']:
        model_path = get_weights_file_path(config, config['preload'])
        print(f"Loading model from {model_path}")
        state = torch.load(model_path)
        initial_epoch = state['epoch'] + 1
        model.load_state_dict(state['model_state_dict'])
        optimizer.load_state_dict(state['optimizer_state_dict'])
        scaler.load_state_dict(state['scaler_state_dict'])
        global_step = state['global_step']

    loss_fn = nn.CrossEntropyLoss(ignore_index=src_tokenizer.token_to_id("[PAD]"), label_smoothing=0.1).to(device)

    for epoch in range(initial_epoch, config['num_epochs']):
        model.train()
        batch_iterator = tqdm(train_loader, desc=f"Processing epoch {epoch}")
        total_loss = 0

        for batch_idx, batch in enumerate(batch_iterator):
            # Update learning rate based on warmup schedule
            if global_step < warmup_steps:
                lr = base_lr * (global_step + 1) / warmup_steps
                for param_group in optimizer.param_groups:
                    param_group['lr'] = lr
            
            encoder_input = batch['encoder_input'].to(device)
            decoder_input = batch['decoder_input'].to(device)
            encoder_mask = batch['encoder_mask'].to(device)
            decoder_mask = batch['decoder_mask'].to(device)
           
            # Use autocast for mixed precision training
            with torch.amp.autocast(device.type):
                encoder_output = model.encode(encoder_input, encoder_mask)
                decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)
                proj_output = model.project(decoder_output)

                label = batch['label'].to(device)

                loss = loss_fn(proj_output.view(-1, tgt_tokenizer.get_vocab_size()), label.view(-1))
                loss = loss / gradient_accumulation_steps
                total_loss += loss.item()

            # Scale the loss and call backward
            scaler.scale(loss).backward()

            # Only update weights after accumulating enough gradients
            if (batch_idx + 1) % gradient_accumulation_steps == 0:
                # Unscale gradients before clipping
                scaler.unscale_(optimizer)
                
                # Clip gradients
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
                
                # Perform optimizer step with GradScaler
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

                
                batch_iterator.set_postfix({"loss": f"{total_loss:6.3f}"})
                writer.add_scalar('Loss/train', total_loss, global_step)
                writer.flush()
                total_loss = 0
                global_step += 1

        model_filename = get_weights_file_path(config, f'{epoch:02d}')
        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scaler_state_dict': scaler.state_dict(),  # Save scaler state
            'global_step': global_step
        }, model_filename)

        run_test(model, test_loader, src_tokenizer, tgt_tokenizer, config['seq_len'], device, lambda msg: batch_iterator.write(msg))

if __name__ == '__main__':
    config = get_config()
    train_model(config)
